{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7c7375",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f1060b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e00a1e12fe44d27a7412d0f7a232dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd266d39612481685b3534241f80328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/803 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c39deb9b7f4826a672af71b2f7cc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eb6fa3e68e474ba63847ca13b56a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d2b05564fb44e1b88aed05164df130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerdurden/miniconda3/envs/tf/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1277: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6567e14a6974b70b90a9950d0a6d017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('SIC98/GPT2-python-code-generator')\n",
    "model = AutoModelWithLMHead.from_pretrained('SIC98/GPT2-python-code-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ab7b83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [15496, 995], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [18435, 995], 'attention_mask': [1, 1]}\n",
      "[15496, 995]\n",
      "[18435, 995]\n",
      "Hello world\n",
      " Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Hello world\"))\n",
    "print(tokenizer(\" Hello world\"))\n",
    "\n",
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.encode(\" Hello world\"))\n",
    "\n",
    "print(tokenizer.decode([15496, 995]))\n",
    "print(tokenizer.decode([18435, 995]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7cc4c83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mInstruction: Generate a simple train loop inpytorch.\u001b[39m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgpt\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(input_ids,\n\u001b[1;32m      7\u001b[0m                              do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                              temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m      9\u001b[0m                              max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     10\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Instruction: Generate a simple train loop inpytorch.\n",
    "\n",
    "Answer: \"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cpu')\n",
    "\n",
    "generated_ids = m.generate(input_ids,\n",
    "                             do_sample=True,\n",
    "                             temperature=0.9,\n",
    "                             max_length=256)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52894ebe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python transformers/examples/text-generation/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path='SIC98/GPT2-python-code-generator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8825d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367c0dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
